{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77d40713",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a href='http://www.pieriandata.com'><img src='../Pierian_Data_Logo.png'/></a>\n",
    "___\n",
    "<center><em>Copyright by Pierian Data Inc.</em></center>\n",
    "<center><em>For more information, visit us at <a href='http://www.pieriandata.com'>www.pieriandata.com</a></em></center>\n",
    "\n",
    "# Keras RL DQN on Image Environment - Exercise - Solutions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748bb80c",
   "metadata": {},
   "source": [
    "In thise notebook you will implement a DQN agent on the famous game of Pong:\n",
    "\n",
    "**Pong-v0**\n",
    "(https://gym.openai.com/envs/Pong-v0/) <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd41d1b",
   "metadata": {},
   "source": [
    "**TASK: Import necessary libraries and create the environment. Also extract the possible actions** <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02250a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "# https://github.com/keras-rl/keras-rl/blob/master/examples/dqn_atari.py\n",
    "\n",
    "\n",
    "from PIL import Image  # To transform the image in the Processor\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.utils import play\n",
    "\n",
    "# Convolutional Backbone Network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Keras-RL\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce274e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "nb_actions = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbcf4ef",
   "metadata": {},
   "source": [
    "**TASK: Play the game manually (keys: a and d to move the bars)** <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f841fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "play.play(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7caea96",
   "metadata": {},
   "source": [
    "**TASK: Define an input size and the window length** <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f0b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d3ae0",
   "metadata": {},
   "source": [
    "**TASK: Create the ImageProcessor** <br />\n",
    "It needs to:\n",
    "1. Resize the image\n",
    "2. Convert it to grayscale\n",
    "3. Standardize it\n",
    "4. Be memory efficient\n",
    "\n",
    "Dont forget the reward clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f05318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        # First convert the numpy array to a PIL Image\n",
    "        img = Image.fromarray(observation)\n",
    "        # Then resize the image\n",
    "        img = img.resize(IMG_SHAPE)\n",
    "        # And convert it to grayscale  (The L stands for luminance)\n",
    "        img = img.convert(\"L\")\n",
    "        # Convert the image back to a numpy array and finally return the image\n",
    "        img = np.array(img)\n",
    "        return img.astype('uint8')  # saves storage in experience memory\n",
    "    \n",
    "    def process_state_batch(self, batch):\n",
    "\n",
    "        # We divide the observations by 255 to compress it into the intervall [0, 1].\n",
    "        # This supports the training of the network\n",
    "        # We perform this operation here to save memory.\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60437e67",
   "metadata": {},
   "source": [
    "**TASK: Design the Convolutional Neural Network** <br />\n",
    "Hint: Make sure to get the right input shape!\n",
    "\n",
    "You can try the same architecture than presented in the previous notebook:\n",
    "1. Conv2D(filters=32, kernel_size=8, stride=4)\n",
    "2. Conv2D(filters=64, kernel_size=4, stride=2)\n",
    "3. Conv2D(filters=64, kernel_size=3, stride=1)\n",
    "4. Dense(512)\n",
    "\n",
    "Dont forget the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8cf28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (WINDOW_LENGTH, IMG_SHAPE[0], IMG_SHAPE[1])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "\n",
    "model.add(Convolution2D(32, (8, 8), strides=(4, 4),kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (4, 4), strides=(2, 2), kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (3, 3), strides=(1, 1), kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a918dce",
   "metadata": {},
   "source": [
    "**TASK: Create the Replay Memory** <br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ab5fd9",
   "metadata": {},
   "source": [
    "**TASK: Create the processor** <br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67790dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ImageProcessor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80ef47c",
   "metadata": {},
   "source": [
    "**TASK: Define the action selection policy.** <br />\n",
    "Feel free to try all policies you like. (Hint: decaying epsilon greedy also works here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ea99cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
    "                              nb_steps=1000000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a87d0f",
   "metadata": {},
   "source": [
    "**TASK: Create the agent.** <br />\n",
    "Dont forget to compile!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf735700",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
    "               processor=processor, nb_steps_warmup=50000, gamma=.99, target_model_update=10000,\n",
    "              train_interval=4, delta_clip=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873d8f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.compile(Adam(lr=.00025), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f219e4",
   "metadata": {},
   "source": [
    "**TASK: Define a checkpoint callback to store the weights during training.** <br />\n",
    "Please name it differently than our provided checkpoint to avoid overwriting it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0ed83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_filename = 'dqn_pong_weights_student.h5f'\n",
    "checkpoint_weights_filename = 'dqn_' + \"pong\" + '_weights_student_{step}.h5f'\n",
    "checkpoint_callback = ModelIntervalCheckpoint(checkpoint_weights_filename, interval=100000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554a8fa3",
   "metadata": {},
   "source": [
    "**TASK: Train the agent.** <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7026da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.fit(env, nb_steps=1500000, callbacks=[checkpoint_callback], log_interval=10000, visualize=False)\n",
    "\n",
    "# After training is done, we save the final weights one more time.\n",
    "dqn.save_weights(weights_filename, overwrite=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13518dee",
   "metadata": {},
   "source": [
    "**TASK: Evaluate the agent.** <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af526ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ed7673",
   "metadata": {},
   "source": [
    "**TASK: Load your weights (or the provided ones) and create an agent from those** <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c3f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights\n",
    "model.load_weights(\"weights_exercise/dqn_PONG_weights_1500000.h5f\")\n",
    "\n",
    "\n",
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1, value_min=.1, value_test=.05,\n",
    "                              nb_steps=100000)\n",
    "\n",
    "processor = ImageProcessor()\n",
    "\n",
    "# Initialize the DQNAgent with the new model and updated policy and compile it\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
    "               processor=processor, nb_steps_warmup=50000, gamma=.99, target_model_update=10000)\n",
    "dqn.compile(Adam(lr=.00025), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34592b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc35f880",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
