{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a href='http://www.pieriandata.com'><img src='../Pierian_Data_Logo.png'/></a>\n",
    "___\n",
    "<center><em>Copyright by Pierian Data Inc.</em></center>\n",
    "<center><em>For more information, visit us at <a href='http://www.pieriandata.com'>www.pieriandata.com</a></em></center>\n",
    "\n",
    "# Processing Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will learn the steps of image preprocessing that are necessary for a deep agent to work properly on images. <br />\n",
    "As already mentioned in the last lecture, when dealing with image data, we often need to store multiple sequential frames in order to feed all available data to the agent. <br />\n",
    "It is not possible to decide whether a ball moves to the left or to the right given only a single image. <br />\n",
    "keras-rl provides a class called **Processor** which is used to process the image data before it gets fed into the backbone network\n",
    "\n",
    "Let us first start by importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "# https://github.com/keras-rl/keras-rl/blob/master/examples/dqn_atari.py\n",
    "\n",
    "from PIL import Image  # To handle images\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.utils import play  # to play manually\n",
    "\n",
    "from rl.core import Processor  # To process the image within the keras-rl training routine\n",
    "from rl.memory import SequentialMemory  # To store the sequential frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first create an image based environment, namely the famous game called **Breakout** (https://gym.openai.com/envs/Breakout-v0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Breakout-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we can also play the game manually. <br />\n",
    "Feel free to try it out: <br />\n",
    "Use the **a** and **d** key to move the bar and space to start the round<br />\n",
    "(Notice how for the image based atari environments we can directly use a **play** function provided by gym to play the game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play.play(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to decide how many sequential frames are necessary to capture all information.<br/>\n",
    "Let's go with 3.<br/>\n",
    "This is also called window_length, which we need to pass to the **SequentialMemory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_LENGTH = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What replay memory effectively does, is that it appends WINDOW_LENGTH sequential frames to a list and then appends this whole list to the memory.\n",
    "Thus a single element from our SequentialMemory contains WINDOW_LENGTH (consecutive images) <br />\n",
    "The code snipped below demonstrates a basic implementation of this routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "env.reset()\n",
    "\n",
    "sequential_frame_buffer = []  # Our actual memory\n",
    "\n",
    "# Temporary storage to capture sequential frames which can store a max of WINDOW_LENGTH images\n",
    "temp_sequential_frames = deque(maxlen=WINDOW_LENGTH)\n",
    "\n",
    "for i in range(10):\n",
    "    action = 3  # always go left, to visualize the movement (action 3)\n",
    "    observation, r, d, info = env.step(action)  # and perform it on the environment to get the next state\n",
    "    \n",
    "    # We have to wait until the deque is full (so it contains exactly WINDOW_LENGTH images)\n",
    "    if len(temp_sequential_frames) == WINDOW_LENGTH: \n",
    "        # If the deque is full we know that it contains WINDOW_LENGTH frames and we append those frames\n",
    "        # to our actual memory\n",
    "        sequential_frame_buffer.append(list(temp_sequential_frames))\n",
    "    \n",
    "    # Update the deque\n",
    "    temp_sequential_frames.append(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the consecutive images stored in each timestep. <br />\n",
    "Each row visualizes one element of our *sequential_frame_buffer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig, axis = plt.subplots(4, WINDOW_LENGTH, figsize=(12, 12))\n",
    "\n",
    "for global_index, timestep in enumerate(sequential_frame_buffer[:4]):\n",
    "    for frame_index, frame in enumerate(timestep):\n",
    "        axis[global_index][frame_index].imshow(frame)\n",
    "\n",
    "fig.subplots_adjust(wspace=0, hspace=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we do not have to do this by ourselves as keras-rl does it automatically if WINDOW_LENGTH is larger than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does the same thing in a much more optimized way\n",
    "memory = SequentialMemory(limit=1000, window_length=WINDOW_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides that we need to decide how large our images should be. <br />\n",
    "Larger images might contain more information but also increase the training time. <br />\n",
    "Let us use an image size of $84\\times84$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (84, 84)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create image processing class, which inherits the **Processor** provided by keras-rl (https://github.com/keras-rl/keras-rl/blob/master/rl/core.py#L515)\n",
    "\n",
    "Each processor can define the following methods:\n",
    "1. *process_step(observation, reward, done, info)* (\"Processes an entire step by applying the processor to the observation, reward, and info arguments\". Used during inference)\n",
    "2. *process_observation(observation)* (\"Processes the observation as obtained from the environment for use in an agent and returns it\")\n",
    "3. *process_reward(reward)* (\"Processes the reward as obtained from the environment for use in an agent and returns it\")\n",
    "4. *process_info(info)* (\"Processes the info as obtained from the environment for use in an agent and returns it)\n",
    "5. *process_action(action)* (\"Processes an action predicted by an agent but before execution in an environment\")\n",
    "6. *process_state_batch(batch)* (\"Processes an entire batch of states and returns it\". Used for training)\n",
    "\n",
    "The overall use of the processor is to act as a \"translator\" which translates the observation provided by gym into something our network can handle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our first processor, called **BreakOutProcessor**\n",
    "As we do not train in this notebook, we only overwrite the *process_observation* function. <br />\n",
    "This function has to perform two operations:\n",
    "1. Resize the image to our desired shape\n",
    "2. Convert it to grayscale (as the colored images do not yield any more information in this case)\n",
    "We will use PIL to perform those tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreakOutProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        # First convert the numpy array to a PIL Image\n",
    "        img = Image.fromarray(observation)\n",
    "        # Then resize the image\n",
    "        img = img.resize(IMG_SHAPE)\n",
    "        # And convert it to grayscale  (The L stands for luminance)\n",
    "        img = img.convert(\"L\")\n",
    "        # Finally we convert the image back to a numpy array and return it\n",
    "        return np.array(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try the processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images = []\n",
    "breakout_proc = BreakOutProcessor()\n",
    "env.reset()\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample()  # sample a random action\n",
    "    observation, r, d, info = env.step(action)  # and perform it on the environment to get the next state\n",
    "    processed_observation = breakout_proc.process_observation(observation)\n",
    "    sample_images.append(processed_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The shape of the original observation is {observation.shape} \" \\\n",
    "        f\"and the shape of the processed observation is {processed_observation.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(sample_images[-1], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a full image processing pipeline which we can use in the next notebook to process the images for training "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
